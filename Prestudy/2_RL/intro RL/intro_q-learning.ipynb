{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6ec5aa4507a8f918",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "<center>\n",
    "\n",
    "# Introduction to Reinforcement Learning\n",
    "\n",
    "## Getting Home through Q-Learning\n",
    "\n",
    "</center>\n",
    "\n",
    "- - -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d0ce19473bf6c277",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "try:\n",
    "    import numpy as np\n",
    "except ImportError:\n",
    "    print(\"You need to install numpy! Open a command prompt and run 'pip install numpy'\")\n",
    "\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "except ImportError:\n",
    "    print(\"You need to install matplotlib! Open a command prompt and run 'pip install matplotlib'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-14d9e8d5cc01d1b4",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "- - -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-141c000a5ea6f1d4",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "After having secured a master's thesis project at Zenseact, you call your friends to share the news of this unique opportunity. All agree that this needs to be celebrated accordingly, and so you meet up at Andra Långgatan. Your most interesting thesis project is cause for a long and jolly evening, and when you finally decide to go home, it has become quite dark already. To make matters even worse, a strong wind is blowing from south/south-west, and you seem to have forgotten the way to your home in Gamlestaden (for whatever reason - blame it on the long day thinking about falsification of autonomous driving controllers if you'd like). You conclude that your best option would be to start walking in some direction in the hope of finding your home eventually. But careful! If you walk too close alongside the Göta Älv, the strong wind might blow you into the river. In addition to the unpleasant experience, the river will take you back to Järntorget and you need to start your journey home all over again. And if that wasn't bad enough yet, you might get convinced to join some _\"late-night studying\"_ if you pass by J. A. Pripps at Chalmers.   \n",
    "\n",
    "In this small assignment, you will implement a Reinforcement Learning algorithm called Q-learning. Reinforcement Learning uses data sampled from the plant (or the environment in RL terms) to derive an optimal controller - just right for finding your way back home.\n",
    "\n",
    "Let us look at the environment first. As part of the task, you will need to decide how much a visit to J.A. Pripps is worth to you. Note that this is strictly speaking not an element of the Q-learning algorithm, but rather an element of the reward function. While many publications on RL assume that the reward function is given, in practice, designing a good reward function, that enables the RL agent to learn the wanted behavior, is a difficult task. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f71764c987c4effe",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import random \n",
    "from time import sleep\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from util import set_up_GBG, plot_pos, plot_Q, plot_average_r, clear_plot\n",
    "\n",
    "\n",
    "class WindyGothenburg(object):\n",
    "\n",
    "    def __init__(self, pripps_reward=1.828, test=False):\n",
    "        \"\"\"The constructor of the grid world WindyGothenburg\"\"\"\n",
    "        self.w = 12\n",
    "        self.h = 12\n",
    "        self.states = {(i, j) for i in range(self.w + 1) for j in range(self.h + 1)}\n",
    "        self.actions = {'north', 'east', 'south', 'west'}\n",
    "        \n",
    "        self._jarntorget = (0, self.h)\n",
    "        self._home = (self.w, self.h)\n",
    "        self._gota_alv = {(k, self.h) for k in range(2, self.w)}\n",
    "        self._chalmers = (3, 2)\n",
    "        assert 0.01 <= pripps_reward <= 10\n",
    "        self._pripps_reward = pripps_reward\n",
    "        self._current_state = self._jarntorget    \n",
    "        \n",
    "        # for rendering\n",
    "        self._test = test\n",
    "        if not self._test:\n",
    "            self.fig, self.gw, self.sp = set_up_GBG(self.w, self.h, self._jarntorget, \n",
    "                                                    self._chalmers, self._home)\n",
    "            self.plot_elems = ()\n",
    "            self.last_pos = None\n",
    "        \n",
    "    def _get_next_state(self, state, action):\n",
    "        \"\"\"The transition function\"\"\"\n",
    "        i, j = state\n",
    "        i = i + 1 if random.random() < 0.05 else i # wind blows you to the east\n",
    "        j = j + 1 if random.random() < 0.1 else j # wind blows you to the north\n",
    "        if action == 'north':\n",
    "            j += 1\n",
    "        elif action == 'east':\n",
    "            i += 1\n",
    "        elif action == 'south':\n",
    "            j -= 1\n",
    "        elif action == 'west':\n",
    "            i -= 1\n",
    "        # ensure the next state is within the grid\n",
    "        i = max(0, min(i, self.w))\n",
    "        j = max(0, min(j, self.h))\n",
    "        return (i, j) # next state\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Advances the simulation by one step\n",
    "        \n",
    "        :param action: the control action to apply\n",
    "        \"\"\"\n",
    "        assert action in self.actions\n",
    "        \n",
    "        next_state = self._get_next_state(self._current_state, action)\n",
    "        self._current_state = next_state\n",
    "        \n",
    "        if next_state == self._home:\n",
    "            reward = 100\n",
    "            done = True\n",
    "        elif next_state in self._gota_alv:\n",
    "            reward = -10\n",
    "            done = True\n",
    "        elif next_state == self._chalmers:\n",
    "            reward = random.choice([0, self._pripps_reward])\n",
    "            done = False\n",
    "        else:\n",
    "            reward = 0\n",
    "            done = False\n",
    "            \n",
    "        return next_state, reward, done\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Must be called if done == True\"\"\"\n",
    "        self._current_state = self._jarntorget\n",
    "        return self._current_state\n",
    "    \n",
    "    def render(self, Q=None, average_reward=None, episode=None):\n",
    "        \"\"\"Renders the environment\"\"\"\n",
    "        assert not self._test, 'Disable test mode for rendering'\n",
    "        if self.last_pos:\n",
    "            self.last_pos.remove()\n",
    "        if self.plot_elems:\n",
    "            clear_plot(*self.plot_elems)\n",
    "        self.last_pos = plot_pos(self.gw, self._current_state)\n",
    "        if Q:\n",
    "            # Plots the contours of max_u Q(*|x) for each x\n",
    "            # Plots also the direction of argmax_u Q(*|x) for each x\n",
    "            self.plot_elems = plot_Q(self.gw, Q, self.w, self.h) \n",
    "        if average_reward:\n",
    "            # Tracking learning over episodes\n",
    "            assert episode is not None, 'Provide the number of the current episode.'\n",
    "            plot_average_r(self.sp, average_reward, episode)\n",
    "        self.fig.canvas.draw()\n",
    "        sleep(0.001) # So you can enjoy the pretty plot\n",
    "    \n",
    "    def close(self):\n",
    "        if not self._test:\n",
    "            plt.close(self.fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-7b0a3e55845aa420",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "The RL algorithm that you will implement is as follows:\n",
    "\n",
    "**Algorithm 1.** Q-learning$(\\alpha, \\epsilon, \\gamma)$\n",
    "\n",
    ">Initialize $Q(x,a)$ arbitrarily\n",
    ">\n",
    ">**for all** episodes **do**\n",
    ">\n",
    ">>Initialize $x$\n",
    ">>\n",
    ">>**for all** steps of episode  **do**\n",
    ">>>\n",
    ">>>Choose $a$ in $x$ using policy derived from $Q$ (e.g. $\\epsilon$-greedy)\n",
    ">>>\n",
    ">>>Take action $u$, observe $r$, $x'$\n",
    ">>>\n",
    ">>>$Q(x,a) = Q(x,a) + \\alpha \\left[r + \\gamma \\max_{a'} Q(x', a') - Q(x,a) \\right]$\n",
    ">>>\n",
    ">>>$x = x'$\n",
    ">>>\n",
    ">>**end for**\n",
    ">\n",
    ">**end for**\n",
    ">\n",
    "**return** $\\pi(a) = argmax_a Q(x, a)$\n",
    "\n",
    "We have already implemented parts of this algorithm for you. Make sure to read carefully the functions below. One of those functions creates the Q-table by using nested Python [dictionaries](https://docs.python.org/3/library/stdtypes.html#mapping-types-dict). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-2939871e3c171dbc",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def initialize_Q(states, actions):\n",
    "    \"\"\"\n",
    "    Initializes the Q-table as a dictionary of dictionaries.\n",
    "    \n",
    "    A particular Q-value can be retrieved by calling Q[x][a].\n",
    "    All actions and their associated values in a state x can \n",
    "    be retrieved through Q[x].\n",
    "    Q-values are initialized to a small random value to encourage \n",
    "    exploration and to facilitate learning.\n",
    "    \n",
    "    :param states: iterable set of states\n",
    "    :param actions: iterable set of actions\n",
    "    \"\"\"\n",
    "    return {x: {a: random.random() * 0.1 for a in actions} for x in states}\n",
    "\n",
    "def argmax_Q(Q, state):\n",
    "    \"\"\"Computes the argmax of Q in a particular state.\"\"\"\n",
    "    max_q = float(\"-inf\")\n",
    "    argmax_q = None\n",
    "    for a, q in Q[state].items():\n",
    "        if q > max_q:\n",
    "            max_q = q\n",
    "            argmax_q = a\n",
    "    return argmax_q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-05ba1591136d06b7",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-cb04ccc104be9b68",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Task 1\n",
    "\n",
    "### Finding Home through Q-learning \n",
    "\n",
    "As a first task, implement a function that chooses with probability $1-\\epsilon$ the action with the highest $Q$-value in a given state (i.e. it chooses greedily), and with probability $\\epsilon$ a random action, where $0 < \\epsilon < 1$. This is a popular exploration strategy in RL that ensures that all states are visited theoretically infinitively often.\n",
    "\n",
    "* Implement the $\\epsilon$-greedy choice in code. \n",
    "* *Hint*: You might want to use the Python function [random.random()](https://docs.python.org/3/library/random.html#https://docs.python.org/3.7/library/random.html#random.random) and [random.choice()](https://docs.python.org/3/library/random.html#random.choice).\n",
    "* *Hint*: You might want to read the documentation of [dictionaries](https://docs.python.org/3/library/stdtypes.html#mapping-types-dict) again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "epsilon_greedy",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def choose_epsilon_greedily(Q, x, epsilon):\n",
    "    \"\"\"\n",
    "    Chooses random action with probability epsilon, else argmax_a(Q(*|x))\n",
    "    \n",
    "    :param Q: Q-table as dict of dicts\n",
    "    :param x: state\n",
    "    :param epsilon: float\n",
    "    \"\"\"\n",
    "    # YOUR SOLUTION HERE\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# space for your own tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-15e9519922bab68d",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Next, we need to implement a function that decides how fast our RL algorithm will be learning. Generally, the learning rate $\\alpha_k$ must satisfy the conditions $\\sum_{k=0}^\\infty \\alpha_k^2 < \\infty$ and $\\sum_{k=0}^\\infty \\alpha_k = \\infty$, to be able to guarantee that the estimates of Q converge to the optimal Q-function.\n",
    "\n",
    "* Implement a function that computes $\\alpha$ given $k$. \n",
    "* *Hint*: Check the lecture notes for further information.\n",
    "\n",
    "A correct implementation of both functions is needed to complete the task. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "alpha",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def get_alpha(k):\n",
    "    \"\"\"\n",
    "    Returns a value of the learning rate.\n",
    "    :param k: integer index of the update \n",
    "    \"\"\"\n",
    "    # YOUR SOLUTION HERE\n",
    "    # E.g. A / (B + k) with some parameters A and B\n",
    "    return alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# space for your own tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "helper_test",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# epsilon-greedy tests\n",
    "Q1 = initialize_Q(states={1}, actions={'a'})\n",
    "assert choose_epsilon_greedily(Q1, 1, 0.1) == 'a'\n",
    "\n",
    "Q2 = initialize_Q(states={1}, actions={'a', 'b'})\n",
    "Q2[1]['a'] = 1\n",
    "Q2[1]['b'] = 0\n",
    "assert choose_epsilon_greedily(Q2, 1, 0.0) == 'a'\n",
    "\n",
    "epsilon = 0.1\n",
    "k = 0\n",
    "l = 0\n",
    "for m in range(1000):\n",
    "    action = choose_epsilon_greedily(Q2, 1, epsilon)\n",
    "    k = k + 1 if action == 'a' else k\n",
    "    l = l + 1 if action == 'b' else l\n",
    "assert k/m >= (1-epsilon)\n",
    "assert l/m > 0.0\n",
    "\n",
    "# learning rate tests\n",
    "assert 0.0 < get_alpha(1) < 1.0\n",
    "assert 0.0 < get_alpha(1000) < 1.0\n",
    "assert 0.0 < get_alpha(1000000) < 1.0\n",
    "assert 0.0 < get_alpha(1000000000) < 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0860dfc12d3aa182",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Finally, we can turn to the $Q$-learning algorithm. We have made a start with it already. \n",
    "\n",
    "* Implement the $Q$-value update from Algorithm 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8f7a3e09b980489f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def learn_q(env, epsilon, gamma, num_episodes=250, max_steps=100, render=False, test=False):\n",
    "    Q = initialize_Q(env.states, env.actions)\n",
    "    k = 0\n",
    "    for l in range(num_episodes):\n",
    "        # Reset for episode\n",
    "        x = env.reset()\n",
    "        done = False\n",
    "        rewards = 0\n",
    "        \n",
    "        for m in range(max_steps):\n",
    "            # Pick action\n",
    "            a = choose_epsilon_greedily(Q, x, epsilon)\n",
    "            next_x, r, done = env.step(a)  \n",
    "            \n",
    "            # Update Q-Table\n",
    "            # YOUR SOLUTION HERE\n",
    "            \n",
    "            # Increment\n",
    "            x = next_x\n",
    "            rewards += r\n",
    "            k += 1\n",
    "            if render:\n",
    "                env.render(Q)\n",
    "            if done:\n",
    "                # Set the Q-values of the terminal state to 0\n",
    "                for action in Q[next_x].keys():\n",
    "                    Q[next_x][action] = 0\n",
    "                break\n",
    "        \n",
    "        # Update plots\n",
    "        if not test:\n",
    "            env.render(Q, rewards/(m+1), l)\n",
    "    \n",
    "    env.close()\n",
    "    return {x: argmax_Q(Q, x) for x in env.states}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-caf9570be22c6c44",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Your task is now to:\n",
    "* play around with the Q-learning for the WindyGothenburg environment and its parameters. In particular:\n",
    " * Choose a value for `pripps_reward`\n",
    " * Choose a value for $\\epsilon$\n",
    " * Choose a value for the discount factor $\\gamma$\n",
    " * Depending on your implementation of `get_alpha`, their might be also parameters to tune there.\n",
    "* Your Q-learning will return the learned policy, which is evaluated in the test cell below. \n",
    "* If your parameters produces policies that get you home in more than 50% of the cases, we are satisfied.  \n",
    "* If this increases to more than 75%, we will be happy.  \n",
    "* _Hint_: If you are ambitious, you will find parameters that have a success rate of 90% and higher. \n",
    "* _Hint_: Make sure to understand deeply our evaluation criteria.\n",
    "* _Hint_: Each parameter affects the learning differently. What values for each parameter would likely produce the wanted behavior? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-fd1bedcc8b202c2b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Choose values for pripps_reward, epsilon, and gamma\n",
    "pripps_reward = None\n",
    "epsilon = None\n",
    "gamma = None\n",
    "\n",
    "# If you would like to watch the episode, set render = True\n",
    "# For updates only at the end of each episode, set render = False\n",
    "# render = False is significantly faster.\n",
    "render = False\n",
    "\n",
    "\"\"\"\n",
    "!!! PLEASE COMMENT OUT THE TWO LINES BELOW AGAIN BEFORE SUBMISSION !!!\n",
    "\"\"\"\n",
    "\n",
    "# env = WindyGothenburg(pripps_reward)\n",
    "# control_policy = learn_q(env, epsilon, gamma, render=render)\n",
    "\n",
    "\"\"\"\n",
    "!!! PLEASE COMMENT OUT THE TWO LINES ABOVE AGAIN BEFORE SUBMISSION !!!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "success_050",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def policy_success_rate(pripps_reward, epsilon, gamma):\n",
    "    solved = 0\n",
    "    for _ in range(200):    \n",
    "        env = WindyGothenburg(pripps_reward, test=True)\n",
    "        control_policy = learn_q(env, epsilon=epsilon, gamma=gamma, num_episodes=250,\n",
    "                                 max_steps=100, render=False, test=True)\n",
    "        x = env.reset()\n",
    "        done = False\n",
    "        rewards = []\n",
    "        i = 0\n",
    "        while not done and i < 20:\n",
    "            x, r, done = env.step(control_policy.get(x))\n",
    "            rewards.append(r)\n",
    "            i += 1\n",
    "        solved += 1 if 100 in rewards else 0\n",
    "    return solved/200\n",
    "\n",
    "rate = policy_success_rate(pripps_reward, epsilon, gamma)\n",
    "assert rate > 0.50, 'Got {} instead'.format(rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "success_075",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert rate > 0.75, 'Got {} instead'.format(rate)\n",
    "print(\"The achieved success rate was \", rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-01abc3c9adfaffd2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-aa0bdaaf74b6e4e9",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Task 2\n",
    "\n",
    "### Reflections on Q-learning \n",
    "\n",
    "* Reflect on this part of the assignment and write down your insights in a few brief sentences. \n",
    "* Make sure to touch upon:\n",
    " * the final value of `pripps_reward` that you chose and why you have chosen that value;\n",
    " * the impact of `pripps_reward` on solving the problem\n",
    " * the final value of $\\epsilon$ that you chose and why you have chosen that value;\n",
    " * the final value of $\\gamma$ that you chose and why you have chosen that value;\n",
    " * the final value/range of $\\alpha$ that you chose and why you have chosen that/those value/s;\n",
    " * the process of finding those parameters;\n",
    " * the contour of the Q-function over time;\n",
    " * and whether you think Q-learning would be a good idea to try next time you are lost at Järntorget. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "RL_reflections",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e53c87aa4cdb8edf",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ca4413be58012d08",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "That is all there is. If you are done,\n",
    "\n",
    "* Save the notebook\n",
    "* Upload the .ipynb file to Canvas\n",
    "* Tell your teammate how much you appreciated their invaluable insights and how fun it was to collaborate with them on the assignments."
   ]
  }
 ],
 "metadata": {
  "author": "",
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
